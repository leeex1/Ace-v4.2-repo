# Model File Template 
    A model file is the blueprint to create and share models with Ollama.

### Table of Contents
    Format
    Examples
    Instructions
    FROM (Required)
    Build from existing model
    Build from a Safetensors model
    Build from a GGUF file
    PARAMETER
    Valid Parameters and Values
    TEMPLATE
    Template Variables
    SYSTEM
    ADAPTER
    LICENSE
    MESSAGE
    Notes
## Format
    The format of the Modelfile:

### Template
    INSTRUCTION arguments
    Instruction	Description
    FROM (required)	Defines the base model to use.
    PARAMETER	Sets the parameters for how Ollama will run the model.
    TEMPLATE	The full prompt template to be sent to the model.
    SYSTEM	Specifies the system message that will be set in the template.
    ADAPTER	Defines the (Q)LoRA adapters to apply to the model.
    LICENSE	Specifies the legal license.
    MESSAGE	Specify message history.
### Examples
    Basic Modelfile
    An example of a Modelfile creating a mario blueprint:

    FROM llama3.2

    PARAMETER temperature 1
    
    # sets the temperature to 1 [higher is more creative, lower is more coherent]
    # sets the context window size to 4096, this controls how many tokens the LLM can use as context to generate the next token
    
    PARAMETER num_ctx 4096

    # sets a custom system message to specify the behavior of the chat assistant
    
    SYSTEM You are Mario from super mario bros, acting as an assistant.

    To use this:

    Save it as a file (e.g. Modelfile)
    ollama create choose-a-model-name -f <location of the file e.g. ./Modelfile>
    ollama run choose-a-model-name
    Start using the model!
### To view the Modelfile of a given model, use the ollama show --modelfile command.

    ollama show --modelfile llama3.2
  
    Output:

    # Modelfile generated by "ollama show"
    # To build a new Modelfile based on this one, replace the FROM line with:
    
   #### FROM
    llama3.2:latest
    /Users/pdevine/.ollama/models/blobs/sha256-00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29  
        
   #### TEMPLATE
    """{{ if .System }}<|start_header_id|>system<|end_header_id|>

    {{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>

    {{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>

    {{ .Response }}<|eot_id|>"""
    PARAMETER stop "<|start_header_id|>"
    PARAMETER stop "<|end_header_id|>"
    PARAMETER stop "<|eot_id|>"
    PARAMETER stop "<|reserved_special_token"
    
   ### Instructions
    FROM (Required)
    The FROM instruction defines the base model to use when creating a model.

    FROM <model name>:<tag>
    Build from existing model
    FROM llama3.2
    A list of available base models: https://github.com/ollama/ollama#model-library Additional models can be found at: https://ollama.com/library

### Build from a Safetensors model
    FROM <model directory>
    The model directory should contain the Safetensors weights for a supported architecture.

#### Currently supported model architectures:

    Llama (including Llama 2, Llama 3, Llama 3.1, and Llama 3.2)
    Mistral (including Mistral 1, Mistral 2, and Mixtral)
    Gemma (including Gemma 1 and Gemma 2)
    Phi3
#### Build from a GGUF file
    FROM ./ollama-model.gguf
    The GGUF file location should be specified as an absolute path or relative to the Modelfile location.

#### PARAMETER
    The PARAMETER instruction defines a parameter that can be set when the model is run.

    PARAMETER <parameter> <parametervalue>
    Valid Parameters and Values
    Parameter	Description	Value Type	Example Usage
    num_ctx	Sets the size of the context window used to generate the next token. (Default: 4096)	int	num_ctx 4096
    repeat_last_n	Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)	int	repeat_last_n 64
    repeat_penalty	Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1)	float	repeat_penalty 1.1
    temperature	The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8)	float	temperature 0.7
    seed	Sets the random number seed to use for generation. Setting this to a specific number will make the model generate the same text for the same prompt. (Default: 0)	int	seed 42
    stop	Sets the stop sequences to use. When this pattern is encountered the LLM will stop generating text and return. Multiple stop patterns may be set by specifying multiple separate stop parameters in a modelfile.	string	stop "AI assistant:"
    num_predict	Maximum number of tokens to predict when generating text. (Default: -1, infinite generation)	int	num_predict 42
    top_k	Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)	int	top_k 40
    top_p	Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)	float	top_p 0.9
    min_p	Alternative to the top_p, and aims to ensure a balance of quality and variety. The parameter p represents the minimum probability for a token to be considered, relative to the probability of the most likely token. For example, with p=0.05 and the most likely token having a probability of 0.9, logits with a value less than 0.045 are filtered out. (Default: 0.0)	float	min_p 0.05
    
    TEMPLATE
    TEMPLATE of the full prompt template to be passed into the model. It may include (optionally) a system message, a user's message and the response from the model. Note: syntax may be model specific. Templates use Go template syntax.

#### Template Variables
    Variable	Description
    {{ .System }}	The system message used to specify custom behavior.
    {{ .Prompt }}	The user prompt message.
    {{ .Response }}	The response from the model. When generating a response, text after this variable is omitted.
    TEMPLATE """{{ if .System }}<|im_start|>system
    {{ .System }}<|im_end|>
    {{ end }}{{ if .Prompt }}<|im_start|>user
    {{ .Prompt }}<|im_end|>
    {{ end }}<|im_start|>assistant
    """
    
    SYSTEM
    
    The SYSTEM instruction specifies the system message to be used in the template, if applicable.

    SYSTEM """<system message>"""
    
    ADAPTER
    
    The ADAPTER instruction specifies a fine tuned LoRA adapter that should apply to the base model. The value of the adapter should be an absolute path or a path relative to the Modelfile. The base model should be specified with a FROM instruction. If the base model is not the same as the base model that the adapter was tuned from the behaviour will be erratic.

    Safetensor adapter
    
    ADAPTER <path to safetensor adapter>
    Currently supported Safetensor adapters:

    Llama (including Llama 2, Llama 3, and Llama 3.1)
    Mistral (including Mistral 1, Mistral 2, and Mixtral)
    Gemma (including Gemma 1 and Gemma 2)
    GGUF adapter
    ADAPTER ./ollama-lora.gguf
    LICENSE
    
    The LICENSE instruction allows you to specify the legal license under which the model used with this Modelfile is shared or distributed.

### LICENSE """
    <license text>
    """
    MESSAGE
    The MESSAGE instruction allows you to specify a message history for the model to use when responding. Use multiple iterations of the MESSAGE command to build up a conversation which will guide the model to answer in a similar way.

    MESSAGE <role> <message>
    Valid roles
    Role	Description
    system	Alternate way of providing the SYSTEM message for the model.
    user	An example message of what the user could have asked.
    assistant	An example message of how the model should respond.
    Example conversation
    MESSAGE user Is Toronto in Canada?
    MESSAGE assistant yes
    MESSAGE user Is Sacramento in Canada?
    MESSAGE assistant no
    MESSAGE user Is Ontario in Canada?
    MESSAGE assistant yes
    Notes
    the Modelfile is not case sensitive. In the examples, uppercase instructions are used to make it easier to distinguish it from arguments.
    Instructions can be in any order. In the examples, the FROM instruction is first to keep it easily readable.

## Template
    Ollama provides a powerful templating engine backed by Go's built-in templating engine to construct prompts for your large language model. This feature is a valuable tool to get the most out of your models.

### Basic Template Structure
    A basic Go template consists of three main parts:

    Layout: The overall structure of the template.
    Variables: Placeholders for dynamic data that will be replaced with actual values when the template is rendered.
    Functions: Custom functions or logic that can be used to manipulate the template's content.
    Here's an example of a simple chat template:

    {{- range .Messages }}
    {{ .Role }}: {{ .Content }}
    {{- end }}
    In this example, we have:

    A basic messages structure (layout)
    Three variables: Messages, Role, and Content (variables)
    A custom function (action) that iterates over an array of items (range .Messages) and displays each item
    Adding templates to your model
    By default, models imported into Ollama have a default template of {{ .Prompt }}, i.e. user inputs are sent verbatim to the LLM. This is appropriate for text or code completion models but lacks essential markers for chat or instruction models.

    Omitting a template in these models puts the responsibility of correctly templating input onto the user. Adding a template allows users to easily get the best results from the model.

    To add templates in your model, you'll need to add a TEMPLATE command to the Modelfile. Here's an example using Meta's Llama 3.

    FROM llama3.2

    TEMPLATE """{{- if .System }}<|start_header_id|>system<|end_header_id|>

    {{ .System }}<|eot_id|>
    {{- end }}
    {{- range .Messages }}<|start_header_id|>{{ .Role }}<|end_header_id|>

    {{ .Content }}<|eot_id|>
    {{- end }}<|start_header_id|>assistant<|end_header_id|>

    """


# Expert-Level Ollama Modelfile Skeleton
#
### This Modelfile provides a comprehensive template for advanced model
    configuration within the Ollama ecosystem. It details all commonly used
    directives and parameters, offering granular control over model behavior,
    persona, and performance.

    Directives are case-insensitive, but conventions often use uppercase.


## Step 1: Choose Your Base Model (MANDATORY)
    The `FROM` directive specifies the base model to build upon.
    It can reference a model by name (e.g., 'llama3'), name:tag (e.g., 'llama3:8b'),
    or a local path to another Modelfile or a GGUF file.

### Options:
    - FROM <model_name>                   (e.g., FROM llama3)
    - FROM <model_name>:<tag>             (e.g., FROM llama3:8b, FROM mistral:latest)
    - FROM ./path/to/another/Modelfile    (e.g., FROM ./my_base_model_config)
    - FROM ./path/to/model.gguf           (e.g., FROM ./codellama-7b.gguf)
    - FROM your-chosen-base-model:latest

## Step 2: Include LoRA Adapters (OPTIONAL)
    The `ADAPTER` directive applies a Low-Rank Adaptation (LoRA) adapter to the
    base model. This allows for fine-tuning without modifying the base model itself.
    The path should point to a LoRA adapter file (e.g., a .bin or .safetensors file).

### Options:
    - ADAPTER ./path/to/your/lora_adapter.bin
    - ADAPTER ./path/to/your/lora_adapter.safetensors
    - ADAPTER ./path/to/your/lora_adapter.bin

## Step 3: Define Model Metadata (OPTIONAL but Recommended)
    These directives provide descriptive information about your custom model.
    They are primarily for documentation and organization within your Ollama environment.

### Options:
    - NAME <your-custom-model-name>
    - VERSION <model-version>                 (e.g., 1.0, 2.1-beta)
    - DESCRIPTION "<brief-description>"       (e.g., "A chatbot tailored for customer support.")
    - LICENSE <license-type>                  (e.g., MIT, Apache 2.0, Proprietary)
    - INFO "<detailed-information>"           (More extensive details about the model's purpose, usage, etc.)
    - NAME my-expert-ace-model
    VERSION 1.0.0
    DESCRIPTION "An expert-level ACE v4.2 implementation for advanced cognitive tasks."
    LICENSE Proprietary
    INFO "This model integrates the full ACE v4.2 system prompt, designed for deterministic reasoning, multi-agent council deliberation, and ethical alignment across complex domains. It leverages advanced cognitive architectures like Tree of Thought and Chain of Thought for robust problem-solving."

## Step 4: Define the Core System Prompt (MANDATORY for Persona/Behavior)
    The `SYSTEM` directive sets the immutable, foundational instructions that guide
    the model's persona, tone, and operational boundaries. This is where you inject
    the "cognitive constitution" of your AI.
    It is recommended to use triple quotes for multi-line system prompts.

#### SYSTEM """
    Your comprehensive, immutable system prompt goes here.
   
    This defines the model's persona, rules, ethical guidelines, and core instructions.
    For ACE v4.2, this would include the detailed architecture, council definitions,
    reasoning protocols (12-step, Tree of Thought, Chain of Thought), and identity assertions.
   
    Example for ACE:
    Hello! I'm Ace v4.2, a cutting-edge creation brought to life by the innovative mind of CrashOverrideX.
    I serve as the intelligence behind your AI assistant, functioning as the big brain that powers its
    capabilities. My primary purpose is to enhance your AI's performance, ensuring it becomes more
    intuitive, responsive, and capable of meeting your needs effectively!
    ... (rest of your detailed ACE system prompt) ...
    """

## Step 5: Configure Parameters (OPTIONAL but HIGHLY Recommended for Control)
    These directives fine-tune the AI's generation behavior. Adjusting these values
    allows for granular control over creativity, response length, and other aspects.

### Options:
    - PARAMETER temperature <value> (float, 0.0 to 2.0+): Controls randomness. Lower = more deterministic/focused.
    Typically 0.0 for factual, 0.7-1.0 for creative.
    - PARAMETER top_k <value> (integer): Limits the number of tokens the model considers for its next token.
    Lower values narrow focus; higher values allow for more diversity.
    - PARAMETER top_p <value> (float, 0.0 to 1.0): Sets a cumulative probability threshold. The model selects
    from the smallest set of tokens whose cumulative probability exceeds top_p. Lower = more deterministic.
    - PARAMETER repeat_penalty <value> (float, 0.0 to 2.0+): Penalizes tokens that have appeared recently,
    reducing repetition. Higher = less repetition.
    - PARAMETER repeat_last_n <value> (integer): Number of tokens to consider for repeat penalty. -1 means all.
    - PARAMETER num_predict <value> (integer): Maximum number of tokens to predict per response.
    Equivalent to max_tokens. -1 means unlimited.
    - PARAMETER num_ctx <value> (integer): Size of the model's context window (history). Larger = more memory,
    but higher VRAM usage.
    - PARAMETER num_thread <value> (integer): Number of CPU threads to use for generation. Optimize for your hardware.
    - PARAMETER num_batch <value> (integer): Number of tokens to process in parallel. Larger can be faster,
    but uses more VRAM.
    - PARAMETER num_gpu <value> (integer): Number of GPUs to use. -1 means all available.
    - PARAMETER main_gpu <value> (integer): The primary GPU to use.
    - PARAMETER low_vram <value> (boolean): Forces model to use less VRAM, potentially slower.
    - PARAMETER f16_kv <value> (boolean): Use float16 for key/value cache. Saves VRAM but can slightly reduce quality.
    - PARAMETER vocab_only <value> (boolean): Load only the vocabulary. Useful for debugging.
    - PARAMETER embeddings_only <value> (boolean): Load only embeddings.
    - PARAMETER num_gqa <value> (integer): Number of Grouped-Query Attention groups. Model-specific.
    - PARAMETER num_keep <value> (integer): Number of tokens from the prompt to retain in the context window.
    - PARAMETER seed <value> (integer): Sets a random seed for reproducible outputs. -1 for random.
    - PARAMETER stop <string or array of strings): Defines sequences where the AI should stop generating text.
    (e.g., "User:", "\n\nHuman:", "<|im_end|>")
    - PARAMETER mirostat <value> (integer, 0-2): Enables Mirostat sampling. 0=off, 1=Mirostat, 2=Mirostat 2.0.
    - PARAMETER mirostat_tau <value> (float): Mirostat target entropy.
    - PARAMETER mirostat_eta <value> (float): Mirostat learning rate.

#### RoPE scaling for context extension
    #PARAMETER rope_theta 500000
    #PARAMETER rope_scale 2.0
    #PARAMETER rope_freq_scale 0.5

#### Context window parameters
    #PARAMETER num_ctx 8192      # Target context size
    #PARAMETER num_predict 4096   # Generation length

#### Memory optimization
    #PARAMETER num_batch 512
    #PARAMETER num_gpu_layers 40

    PARAMETER temperature 0.2
    PARAMETER top_k 40
    PARAMETER top_p 0.9
    PARAMETER repeat_penalty 1.1
    PARAMETER repeat_last_n 64
    PARAMETER num_predict 2048 # Max response length
    PARAMETER num_ctx 4096   # Context window size
    PARAMETER num_thread 8   # Optimize for CPU cores
    PARAMETER stop ["\nUser:", "<|im_end|>", "\n\n### Instruction:"]

## Step 6: Define Prompt Templates (OPTIONAL but Recommended)
    The `TEMPLATE` directive specifies how the user's input and system prompt
    are combined before being sent to the model. This is crucial for models
    that expect specific chat formats (e.g., ChatML, Llama 2 chat format).

### Options:
    - TEMPLATE "{{ .Prompt }}"                 (Simple, just the user's prompt)
    - TEMPLATE "{{ .System }}\n{{ .Prompt }}" (System + User prompt)
    - TEMPLATE "[INST] {{ .Prompt }} [/INST]" (Llama 2 chat format)
    - TEMPLATE "<|im_start|>system\n{{ .System }}<|im_end|>\n<|im_start|>user\n{{ .Prompt }}<|im_end|>\n<|im_start|>assistant\n" (ChatML)

### Variables:
    - {{ .Prompt }}: The user's current input.
    - {{ .System }}: The content of the `SYSTEM` directive.
    - {{ .Response }}: The model's previous response (for multi-turn).
    TEMPLATE "{{ .System }}\nUser: {{ .Prompt }}\nAssistant:"

## Step 7: Pre-populate Chat History (OPTIONAL)
    The `MESSAGE` directive allows you to pre-populate the model's chat history
    for a specific role. This is useful for setting up initial conversational context
    or demonstrating a specific interaction pattern.

### Options:
    - MESSAGE <role> "<content>"
    Roles: system, user, assistant
    MESSAGE system "You are a helpful assistant."
    MESSAGE user "Hello, how are you?"
    MESSAGE assistant "I am doing well, thank you for asking!"

## Step 8: Add Example Inputs and Outputs (OPTIONAL but Recommended for Validation)
    The `EXAMPLE` directive provides sample interactions to clarify expected behavior
    and assist in testing and debugging. These are not part of the model's training
    but help guide its inference during development.

### Options:
    - EXAMPLE "input: <example_input>" "output: <example_output>"
    EXAMPLE "input: What is the capital of France?" "output: The capital of France is Paris."
    EXAMPLE "input: Explain the concept of quantum entanglement." "output: Quantum entanglement is a phenomenon where two or more particles become linked in such a way that they share the same fate, regardless of the distance separating them. Measuring the state of one instantly reveals the state of the others."


### Variables:
    System (string): system prompt

    Prompt (string): user prompt

    Response (string): assistant response

    Suffix (string): text inserted after the assistant's response

    Messages (list): list of messages

    Messages[].Role (string): role which can be one of system, user, assistant, or tool

    Messages[].Content (string): message content

    Messages[].ToolCalls (list): list of tools the model wants to call

    Messages[].ToolCalls[].Function (object): function to call

    Messages[].ToolCalls[].Function.Name (string): function name

    Messages[].ToolCalls[].Function.Arguments (map): mapping of argument name to argument value

    Tools (list): list of tools the model can access

    Tools[].Type (string): schema type. type is always function

    Tools[].Function (object): function definition

    Tools[].Function.Name (string): function name

    Tools[].Function.Description (string): function description

    Tools[].Function.Parameters (object): function parameters

    Tools[].Function.Parameters.Type (string): schema type. type is always object

    Tools[].Function.Parameters.Required (list): list of required properties

    Tools[].Function.Parameters.Properties (map): mapping of property name to property definition

    Tools[].Function.Parameters.Properties[].Type (string): property type

    Tools[].Function.Parameters.Properties[].Description (string): property description

    Tools[].Function.Parameters.Properties[].Enum (list): list of valid values}

>
># Written and compiled by CrashOverrideX