models:
  # Mini Model (19x1B)
  - model: TheBloke/openchat-1b
    parameters:
      weight: 0.8
  - model: ehartford/dolphin-1b
    parameters:
      weight: 0.8
  - model: jondurbin/airoboros-1b
    parameters:
      weight: 0.8
  # ... (add remaining 1B models)

  # Base Model (19x4B)
  - model: lmsys/vicuna-4b
    parameters:
      weight: 1.0
  - model: TheBloke/deepseek-4b
    parameters:
      weight: 1.0
  - model: openchat/openchat-4b
    parameters:
      weight: 1.0
  # ... (add remaining 4B models)

  # Reasoning Model (19x8B)
  - model: deepseek-ai/deepseek-8b
    parameters:
      weight: 1.2
  - model: TheBloke/phi-2-8b
    parameters:
      weight: 1.2
  - model: lmsys/vicuna-8b
    parameters:
      weight: 1.2
  # ... (add remaining 8B models)

merge_method: "dare_ties" # Using DARE-ties merge strategy
dtype: "bfloat16"
target_model: "/workspaces/Ace-v4.2-repo/ace-moe"

# Optimization settings
optimization:
  mode: "moe"  # Mixture of Experts mode
  num_experts: 19
  expert_size: "auto"
  router_type: "hash"  # Hash-based routing for better distribution
  balance_alpha: 0.01  # Balance between specialization and load
